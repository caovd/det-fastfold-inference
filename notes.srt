bash inference.sh 14507 8 64
RuntimeError: CUDA out of memory. Tried to allocate 50.96 GiB (GPU 7; 79.21 GiB total capacity; 60.10 GiB already allocated; 17.51 GiB free; 60.13 GiB reserved in total by PyTorch) 

bash inference.sh 14507 8 8
RuntimeError: CUDA out of memory. Tried to allocate 50.96 GiB (GPU 5; 79.21 GiB total capacity; 60.10 GiB already allocated; 17.37 GiB free; 60.13 GiB reserved in total by PyTorch)

bash inference.sh 13477 8 8
RuntimeError: CUDA out of memory. Tried to allocate 43.98 GiB (GPU 7; 79.21 GiB total capacity; 52.39 GiB already allocated; 25.22 GiB free; 52.42 GiB reserved in total by PyTorch)

bash inference.sh 8797 8 8
RuntimeError: CUDA out of memory. Tried to allocate 36.90 GiB (GPU 0; 79.21 GiB total capacity; 42.99 GiB already allocated; 34.64 GiB free; 43.00 GiB reserved in total by PyTorch)

bash inference.sh 8797 8 64
RuntimeError: CUDA out of memory. Tried to allocate 36.90 GiB (GPU 5; 79.21 GiB total capacity; 42.99 GiB already allocated; 34.50 GiB free; 43.00 GiB reserved in total by PyTorch)

bash inference.sh 7081 8 8
RuntimeError: CUDA out of memory. Tried to allocate 47.82 GiB (GPU 4; 79.21 GiB total capacity; 28.83 GiB already allocated; 37.39 GiB free; 40.12 GiB reserved in total by PyTorch)

bash inference.sh 5890 8 8
RuntimeError: CUDA out of memory. Tried to allocate 11.37 GiB (GPU 7; 79.21 GiB total capacity; 65.14 GiB already allocated; 7.38 GiB free; 70.26 GiB reserved in total by PyTorch)

bash inference.sh 5005 8 8
RuntimeError: CUDA out of memory. Tried to allocate 8.21 GiB (GPU 4; 79.21 GiB total capacity; 61.80 GiB already allocated; 3.49 GiB free; 71.37 GiB reserved in total by PyTorch)

bash inference.sh 4008 8 8
assert numerator % denominator == 0, '{} is not divisible by {}'.format(numerator, denominator)
AssertionError: 132 is not divisible by 8


ModuleNotFoundError: No module named 'colossalai.context.parallel_mode'
->
RUN pip install colossalai==0.2.7
->
caovd/fastfold-1.0:v2

bash inference.sh 1280 2 512
Inference time: 145.31094053096604
Inference time: 145.2150376579957

bash inference.sh 1280 4 512
Inference time: 129.78119478700683
Inference time: 130.0383233570028
Inference time: 130.4622844569385
Inference time: 129.83195168094244

bash inference.sh 1280 8 512
-- Process 2 terminated with the following error:
Traceback (most recent call last):
  File "/opt/conda/lib/python3.9/site-packages/torch/multiprocessing/spawn.py", line 69, in _wrap
    fn(i, *args)
  File "/run/determined/workdir/inference.py", line 151, in inference_model
    out = model(batch)
  File "/opt/conda/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/run/determined/workdir/fastfold/model/hub/alphafold.py", line 522, in forward
    outputs, m_1_prev, z_prev, x_prev = self.iteration(
  File "/run/determined/workdir/fastfold/model/hub/alphafold.py", line 381, in iteration
    m, z, s = self.evoformer.inplace(
  File "/run/determined/workdir/fastfold/model/fastnn/evoformer.py", line 324, in inplace
    m, z = checkpoint_blocks(
  File "/run/determined/workdir/fastfold/utils/checkpointing.py", line 73, in checkpoint_blocks
    return exec(blocks, args)
  File "/run/determined/workdir/fastfold/utils/checkpointing.py", line 60, in exec
    a = wrap(block(*a))
  File "/run/determined/workdir/fastfold/model/fastnn/evoformer.py", line 118, in inplace
    m[0] = scatter(m[0], dim=1)
  File "/run/determined/workdir/fastfold/distributed/comm.py", line 89, in scatter
    input = _split(input, dim=dim)
  File "/run/determined/workdir/fastfold/distributed/comm.py", line 34, in _split
    split_size = divide(tensor.shape[dim], gpc.get_world_size(ParallelMode.TENSOR))
  File "/run/determined/workdir/fastfold/distributed/comm.py", line 14, in divide
    ensure_divisibility(numerator, denominator)
  File "/run/determined/workdir/fastfold/distributed/core.py", line 9, in ensure_divisibility
    assert numerator % denominator == 0, '{} is not divisible by {}'.format(numerator, denominator)
AssertionError: 132 is not divisible by 8

Tested 256.fasta but got same error.

torchrun --nproc_per_node=2 perf.py --msa-length 128 --res-length 256 --dap-size 2
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 9.800 ms Bwd Time / Layer: 13.279 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 9.802 ms Bwd Time / Layer: 13.279 ms

torchrun --nproc_per_node=4 perf.py --msa-length 128 --res-length 256 --dap-size 4
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.052 ms Bwd Time / Layer: 12.332 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.040 ms Bwd Time / Layer: 12.383 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.041 ms Bwd Time / Layer: 12.442 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.056 ms Bwd Time / Layer: 12.347 ms

torchrun --nproc_per_node=8 perf.py --msa-length 128 --res-length 256 --dap-size 8
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.543 ms Bwd Time / Layer: 12.582 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.546 ms Bwd Time / Layer: 12.579 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.519 ms Bwd Time / Layer: 12.593 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.532 ms Bwd Time / Layer: 12.633 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.530 ms Bwd Time / Layer: 12.579 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.520 ms Bwd Time / Layer: 12.579 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.525 ms Bwd Time / Layer: 12.584 ms
[ MSA Attn ] Input:    1,  128,  256, ( 256  128) Fwd Time / Layer: 10.528 ms Bwd Time / Layer: 12.694 ms


22 Jan 2024
bash inference.sh 2564 2 64
Inference time: 495.49669190100394
Inference time: 495.58941866399255

bash inference.sh 2564 4 64
Inference time: 261.9764190259157
Inference time: 263.46148512593936
Inference time: 262.15494699194096
Inference time: 261.94916975090746

bash inference.sh 3013 2 64
Inference time: 1368.7570391899208
Inference time: 1368.7416186690098

bash inference.sh 3013 4 64
Inference time: 419.00342344795354
Inference time: 420.6114995650714
Inference time: 419.036864168942
Inference time: 419.32677192997653


bash inference.sh 3507 4 64 -> OOM
->
bash inference.sh 3507 4 16
Inference time: 677.8875575800193
Inference time: 679.3543770259712
Inference time: 677.8916596180061
Inference time: 678.0930635270197

bash inference.sh 3507 2 16
Inference time: 1426.2013682859251
Inference time: 1426.1477607189445

bash inference.sh 4008 4 16
Inference time: 1500.0108950060094
Inference time: 1492.8197148339823
Inference time: 1492.7698704940267
Inference time: 1492.9790755610447

bash inference.sh 4008 2 16
Inference time: 2872.9161298270337
Inference time: 2873.3120301369345


bash inference.sh 5005 4 16 -> OOM
bash inference.sh 5005 4 8 -> OOM
